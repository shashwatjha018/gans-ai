{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import torch\n", "from torch import nn, optim\n", "from facenet_pytorch import InceptionResnetV1\n", "import pdb\n", "import utils\n", "import time\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["TODO: add \"returns\" to docstrings<br>\n", "TODO: unify naming conventions across all functions--the same variables are often being passed to different functions under different names<br>\n", "TODO: add unit tests"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Generator(nn.Module):\n", "    def __init__(self, learning_rate=1e-3, weight_decay=1e-3, load_path=None):\n", "        \"\"\"Initialize generator for sampling facemask designs from latent space.\n", "        \n", "        # Arguments\n", "        * `learning_rate` - learning rate used during parameter update step, defaults to 1e-3\n", "        * `weight_decay` - weight on the L2 regularization term during optimization, defaults to 1e-3\n", "        * `load_path` - optional path to load existing weights from, defaults to None\n", "        \"\"\"\n", "        super(Generator, self).__init__()\n", "        self.input_dim = 100\n", "        self.output_dim = (3, 128, 128)\n", "        self.learning_rate = learning_rate\n\n", "        # network layers\n", "        # input: 100-dim latent vector\n", "        # output: 64x64x3 RGB image\n", "        self.h1 = nn.Sequential(\n", "            nn.Linear(self.input_dim, 32),\n", "            nn.ReLU(),\n", "        )\n", "        self.h2 = nn.Sequential(\n", "            nn.Linear(32, 32),\n", "            nn.ReLU(),\n", "        )\n", "        self.output = nn.Sequential(\n", "            nn.Linear(32, np.prod(self.output_dim)),\n", "            nn.Sigmoid(),\n", "        )\n\n", "        # optimization\n", "        self.optimizer = optim.Adam(\n", "            self.parameters(), lr=learning_rate, weight_decay=weight_decay\n", "        )\n\n", "        # loss (cross entropy)\n", "        self.criterion = nn.CrossEntropyLoss()\n\n", "        # load prior weights\n", "        if load_path:\n", "            self.load_state_dict(torch.load(load_path))\n", "    def forward(self, latent_=None):\n", "        \"\"\"Generate a facemask design from the sampled latent space.\n", "        \n", "        # Arguments\n", "        * `latent_` - optional latent sample, defaults to a 100x1 vector of\n", "            random numbers sampled uniformly from [0, 1)\n", "        \"\"\"\n", "        latent = latent_ if latent_ is not None else torch.rand(1, 100)\n", "        y = self.h1(latent)\n", "        y = self.h2(y)\n", "        output = self.output(y)\n", "        output = torch.reshape(output, self.output_dim)\n", "        return output"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Projector(nn.Module):\n", "    def __init__(self, learning_rate=1e-3, load_path=None, device=None):\n", "        \"\"\"Initialize projector for projecting facemask designs onto images of individuals.\n", "        \n", "        # Arguments\n", "        * `learning_rate` - learning rate used during parameter update step, defaults to 1e-3\n", "        * `load_path` - optional path to load existing weights from, defaults to None\n", "        * `device` - optional PyTorch device to run project on (e.g. CUDA, CPU), defaults to None\n", "        \"\"\"\n", "        super(Projector, self).__init__()\n", "        self.device = device\n\n", "        # hyperparameters\n", "        channels = [3, 6, 12, 18, 24, 1]\n\n", "        # layers\n", "        # input: 128x128 RGB image\n", "        # output: 128x128 transparency mask\n", "        self.h1 = nn.Sequential(\n", "            nn.Conv2d(\n", "                in_channels=channels[0],\n", "                out_channels=channels[1],\n", "                kernel_size=3,\n", "                padding=1,\n", "            ),\n", "            nn.ReLU(),\n", "        )\n", "        self.h2 = nn.Sequential(\n", "            nn.Conv2d(\n", "                in_channels=channels[1],\n", "                out_channels=channels[2],\n", "                kernel_size=3,\n", "                padding=1,\n", "            ),\n", "            nn.ReLU(),\n", "        )\n", "        self.h3 = nn.Sequential(\n", "            nn.Conv2d(\n", "                in_channels=channels[2],\n", "                out_channels=channels[3],\n", "                kernel_size=3,\n", "                padding=1,\n", "            ),\n", "            nn.ReLU(),\n", "        )\n", "        self.h4 = nn.Sequential(\n", "            nn.Conv2d(\n", "                in_channels=channels[3],\n", "                out_channels=channels[4],\n", "                kernel_size=3,\n", "                padding=1,\n", "            ),\n", "            nn.ReLU(),\n", "        )\n", "        self.output = nn.Sequential(\n", "            nn.Conv2d(\n", "                in_channels=channels[4],\n", "                out_channels=channels[5],\n", "                kernel_size=3,\n", "                padding=1,\n", "            ),\n", "            nn.Sigmoid(),\n", "        )\n\n", "        # optimizer (Adam)\n", "        self.optimizer = optim.Adam(self.parameters(), learning_rate)\n\n", "        # loss (MSE)\n", "        self.criterion = nn.MSELoss()\n\n", "        # load prior weights\n", "        if load_path:\n", "            self.load_state_dict(torch.load(load_path))\n\n", "        # run on device\n", "        if device:\n", "            self.to(device)\n", "    def forward(self, x):\n", "        \"\"\"One forward propagation of the projector for a batch of images\"\"\"\n", "        y = self.h1(x)\n", "        y = self.h2(y)\n", "        y = self.h3(y)\n", "        y = self.h4(y)\n", "        y = self.output(y)\n", "        return y\n", "    def fit(self, inputs, outputs, num_epochs=10):\n", "        \"\"\"Train projector given masked faces as input and transparency masks as outputs.\n", "        \n", "        # Arguments\n", "        * `inputs` - a batch of faces to train on\n", "        * `outputs` - the ID of every input face\n", "        * `num_epochs` - number of epochs to run, defaults to 10\n", "        \"\"\"\n", "        self.train()\n", "        return utils.fit(self, inputs, outputs, num_epochs=num_epochs)\n", "    def predict(self, x, process=False):\n", "        \"\"\"Predict transparency mask for faces not trained on.\n", "        \n", "        # Arguments\n", "        * `x` - face(s) to generate transparency mask(s) for\n", "        * `process` - whether to post-process the transparency masks, defaults to False\n", "        \"\"\"\n", "        if len(x.shape) < 4:  # pre-process un-batched inputs\n", "            x = torch.unsqueeze(x, 0)\n", "        y = self.forward(x)\n", "        if process:  # post-process outputs to enhance mask quality\n", "            avg = torch.mean(y)\n", "            y = (y > avg).type(torch.IntTensor)\n", "        return y\n", "    def evaluate(self, inputs, correct_outputs):\n", "        \"\"\"Evaluate loss of projector for given inputs\n", "        \n", "        # Arguments\n", "        * `inputs` - a batch of faces to train on\n", "        * `correct_outputs` - the correct ID of every input face\n", "        \"\"\"\n", "        # set model to eval mode\n", "        self.eval()\n\n", "        # forward propagation\n", "        batch_outputs = self.forward(inputs)\n\n", "        # return test loss\n", "        loss = self.criterion(batch_outputs.float(), correct_outputs.float())\n", "        n_examples = correct_outputs.shape[0]\n", "        return float(loss) / n_examples\n", "    def project_mask(self, facemask, masked_faces, process=False):\n", "        \"\"\"Project a facemask design onto images of masked individuals.\n", "        \n", "        # Arguments\n", "        * `facemask` - a 128x128 RGB facemask\n", "        * `masked_faces` - the masked faces to replace with the desired facemask\n", "        * `process` - whether to post-process the transparency masks, defaults to False\n", "        \"\"\"\n", "        transparency_masks = self.predict(masked_faces, process=process)\n", "        if masked_faces.device.type == \"cuda\":\n", "            transparency_masks = transparency_masks.to(\n", "                torch.device(\n", "                    \"{}:{}\".format(masked_faces.device.type, masked_faces.device.index)\n", "                )\n", "            )\n", "        if facemask.shape[2] == 3:\n", "            facemask = torch.transpose(facemask, 2, 1)\n", "            facemask = torch.transpose(facemask, 1, 0)\n", "        facemask = facemask.unsqueeze(0)\n", "        generator_faces = (masked_faces + 0.5) * transparency_masks + facemask * (\n", "            1 - transparency_masks\n", "        )\n", "        return generator_faces"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Discriminator(nn.Module):\n", "    def __init__(self, learning_rate=1e-3, regularization=1e-2, load_path=None):\n", "        \"\"\"Initialize discriminator as pre-trained facial recognition model.\n", "        \n", "        # Arguments\n", "        * `learning_rate` - learning rate used during parameter update step, defaults to 1e-3\n", "        * `regularization` - L1 regularization weight acting on the fine-tuning layer\n", "        * `load_path` - optional path to load existing weights from, defaults to None\n", "        \"\"\"\n", "        super(Discriminator, self).__init__()\n", "        self.model = InceptionResnetV1(pretrained=\"vggface2\").eval()\n\n", "        # set trainable layer for competition against generator\n", "        self.finetune = self.model.last_linear\n", "        self.finetune.train()\n", "        self.optimizer = optim.Adam(self.finetune.parameters(), learning_rate)\n", "        self.criterion = lambda x, y: -1 * nn.CrossEntropyLoss()(\n", "            x, y\n", "        ) + regularization * torch.sum(torch.abs(self.finetune.weight))\n\n", "        # load prior fine-tuning weights\n", "        if load_path:\n", "            self.finetune.load_state_dict(torch.load(load_path))\n", "    def forward(self, img):\n", "        \"\"\"One forward propagation of the discriminator for a batch of images.\n", "        \n", "        # Arguments\n", "        * `img` - a batch of images to perform facial recognition on\n", "        \"\"\"\n", "        return self.model(img)\n", "    def evaluate(self, query, target):\n", "        \"\"\"Evaluate classification accuracy of discriminator for given inputs.\n", "        \n", "        # Arguments\n", "        * `query` - the input images to perform facial recognition on\n", "        * `target` - the \"known\" faces that the queried images are compared to\n", "        \"\"\"\n", "        \n", "        # get embeddings\n", "        target_embeddings = self.forward(target)\n", "        query_embeddings = self.forward(query)\n\n", "        # find distances between each pair of target and query embeddings\n", "        target_embeddings = torch.unsqueeze(target_embeddings, 1)\n", "        query_embeddings = torch.unsqueeze(query_embeddings, 0)\n", "        distances = torch.norm(target_embeddings - query_embeddings, dim=2)\n\n", "        # classify queried images and return accuracy\n", "        num_queries = query.shape[0]\n", "        classifications = torch.argmin(distances, dim=0)\n", "        accuracy = (\n", "            sum(classifications == torch.Tensor([i for i in range(num_queries)]))\n", "            / num_queries\n", "        )\n", "        return accuracy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class GAN:\n", "    def __init__(self, generator, projector, discriminator, device=None):\n", "        \"\"\"\n", "        Initialize GAN\n\n", "        # Arguments        \n", "        * `generator` - generative network for producing mask designs (None -> 128x128 RGB facemask)\n", "        * `projector` - supervised network for projecting mask designs onto images of faces (128x128 RBG image -> 128x128 transparency mask)\n", "        * `discriminator` - facial recognition network for id-ing faces (128x128 RGB image -> 512-dimensional vector embedding)\n", "        \"\"\"\n", "        super(GAN, self).__init__()\n", "        self.generator = generator\n", "        self.projector = projector\n", "        self.discriminator = discriminator\n", "        self.device = device\n", "        self.unmasked_embeddings = None\n\n", "        # send models to device if provided\n", "        if device:\n", "            self.generator.to(device)\n", "            self.projector.to(device)\n", "            self.discriminator.to(device)\n\n", "        # train optimizer and loss on generator alone\n", "        self.optimizer = self.generator.optimizer\n", "        self.criterion = self.generator.criterion\n", "    def compute_unmasked_embeddings(self, unmasked_faces):\n", "        \"\"\"Compute and store database of embeddings for unmasked faces.\n", "        \n", "        # Arguments\n", "        * `unmasked_faces` - a batch of images of unmasked faces\n", "        \"\"\"\n", "        if self.device:\n", "            unmasked_faces = unmasked_faces.to(self.device)\n", "        self.unmasked_embeddings = self.discriminator(unmasked_faces).detach()\n", "        if self.device:\n", "            unmasked_faces = unmasked_faces.to(torch.device(\"cpu\"))\n", "    def generate_mask(self):\n", "        \"\"\"Generate a facemask design.\n", "        \n", "        Returns a 128x128 RGB image of a facemask.\n", "        \"\"\"\n", "        return self.generator()\n", "    def project_mask(self, facemask, masked_faces, process=False):\n", "        \"\"\"Project a facemask design onto images of masked individuals.\n", "        \n", "        # Arguments\n", "        * `facemask` - a 128x128 RGB facemask\n", "        * `masked_faces` - the masked faces to replace with the desired facemask\n", "        * `process` - whether to post-process the transparency masks, defaults to False\n", "        \"\"\"\n", "        return self.projector.project_mask(facemask, masked_faces, process=process)\n", "    def one_hot(self, ids):\n", "        \"\"\"Generate one-hot vectors for given ids based on pre-computed embeddings\"\"\"\n", "        classes = torch.zeros((self.unmasked_embeddings.shape[0], len(ids)))\n", "        for i in range(classes.shape[0]):\n", "            classes[i, ids[i]] = 1\n", "        return classes\n", "    def forward(self, masked_faces):\n", "        \"\"\"One forward propagation of the GAN for a batch of faces.\n", "        \n", "        # Arguments\n", "        * `masked_faces` - a batch of 128x128 RGB images of masked faces\n", "        \"\"\"\n", "        # generate mask design from latent sample\n", "        mask = (\n", "            self.generator(torch.rand(1, 100).to(self.device))\n", "            if self.device\n", "            else self.generator()\n", "        )\n\n", "        # project mask design onto images\n", "        generator_faces = self.project_mask(mask, masked_faces) - 0.5\n\n", "        # get embeddings from discriminator\n", "        generator_embeddings = self.discriminator(generator_faces)\n\n", "        # determine distances from known, unmasked embeddings\n", "        difference = self.unmasked_embeddings.unsqueeze(\n", "            0\n", "        ) - generator_embeddings.unsqueeze(1)\n", "        generator_distances = torch.linalg.norm(difference, dim=2)\n", "        return generator_distances\n", "    def fit(\n", "        self, masked_faces, unmasked_faces, correct_ids, num_epochs=10, verbose=False\n", "    ):\n", "        \"\"\"Train the generator and discriminator against each other simultaneously.\n", "        \n", "        # Arguments\n", "        * `masked_faces` - a batch of 128x128 RGB images of masked faces\n", "        * `unmasked_faces` - a batch of 128x128 RGB images of unmasked faces\n", "        * `correct_ids` - the ID of every face in the batch\n", "        \"\"\"\n", "        # set models to train mode\n", "        self.generator.train()\n", "        self.discriminator.train()\n\n", "        # print loss column headers\n", "        print(\"\\n\\t\\t\\t\\tGenerator \\t\\tDiscriminator\")\n\n", "        # fit on data\n", "        gen_epoch_loss = 0\n", "        dis_epoch_loss = 0\n", "        for epoch in range(1, num_epochs + 1):\n", "            # sort data into minibatches\n", "            masked_faces, unmasked_faces, correct_ids = self.shuffle_data(\n", "                masked_faces, unmasked_faces, correct_ids\n", "            )\n", "            minibatches = self.batch_data(masked_faces, unmasked_faces, correct_ids)\n\n", "            # train on each minibatch\n", "            gen_epoch_loss = 0\n", "            dis_epoch_loss = 0\n", "            for batch in minibatches:\n", "                gen_loss, dis_loss = self.train_batch(batch)\n", "                gen_epoch_loss += gen_loss\n", "                dis_epoch_loss += dis_loss\n", "            gen_epoch_loss /= len(minibatches)\n", "            dis_epoch_loss /= len(minibatches)\n\n", "            # verbose output\n", "            if verbose and (epoch % 10 != 0):\n", "                print(\n", "                    \"Epoch {} losses: \\t\\t{} \\t\\t{}\".format(\n", "                        epoch, gen_epoch_loss, dis_epoch_loss\n", "                    )\n", "                )\n\n", "            # output loss and save temp results\n", "            if epoch % 10 == 0:\n", "                # print\n", "                print(\n", "                    \"Epoch {} losses: \\t\\t{} \\t\\t{}\".format(\n", "                        epoch, gen_epoch_loss, dis_epoch_loss\n", "                    )\n", "                )\n", "                # save models\n", "                save_dir = \"../models/temp\"\n", "                self.save(save_dir, epoch)\n", "                # save mask\n", "                mask = self.generator()\n", "                mask = mask.detach().numpy()\n", "                mask = np.transpose(mask, (1, 2, 0))\n", "                plt.figure()\n", "                plt.imshow(mask)\n", "                plt.savefig(\"../figures/mask_evolution/{}.png\".format(epoch))\n", "    def shuffle_data(self, masked, unmasked, outputs):\n", "        \"\"\"Shuffle the first dimension of a set of input/output data.\n", "        \n", "        # Arguments\n", "        * `masked` - a batch of 128x128 RGB images of masked faces\n", "        * `unmasked` - a batch of 128x128 RGB images of unmasked faces\n", "        * `outputs` - the ID of every face in the batch\n", "        \"\"\"\n", "        n_examples = outputs.shape[0]\n", "        shuffled_indices = torch.randperm(n_examples)\n", "        masked = masked[shuffled_indices]\n", "        unmasked = unmasked[shuffled_indices]\n", "        outputs = outputs[shuffled_indices]\n", "        return masked, unmasked, outputs\n", "    def batch_data(self, masked, unmasked, outputs, batch_size=16):\n", "        \"\"\"Convert full input/output pairs to a list of batched tuples.\n", "        \n", "        Returns a list of batches formatted as a (masked faces, unmasked faces, face IDs) tuple.\n", "        \n", "        # Arguments\n", "        * `masked` - a set of 128x128 RGB images of masked faces\n", "        * `unmasked` - a set of 128x128 RGB images of unmasked faces\n", "        * `outputs` - the ID of every face in the set\n", "        * `batch_size` - the number of images to put in each batch, defaults to 16\n", "        \"\"\"\n", "        n_examples = outputs.shape[0]\n", "        return [\n", "            (\n", "                masked[batch_size * i : batch_size * (i + 1)],\n", "                unmasked[batch_size * i : batch_size * (i + 1)],\n", "                outputs[batch_size * i : batch_size * (i + 1)],\n", "            )\n", "            for i in range(n_examples // batch_size)\n", "        ]\n", "    def train_batch(self, batch):\n", "        \"\"\"Perform one iteration of model training given a single batch.\n", "        \n", "        # Arguments\n", "        * `batch` - a training batch formatted as a (masked faces, unmasked faces, face IDs) tuple\n", "        \"\"\"\n", "        # send data to CUDA if necessary\n", "        masked_faces, unmasked_faces, correct_ids = batch\n", "        if self.device:\n", "            masked_faces = masked_faces.to(self.device)\n", "            unmasked_faces = unmasked_faces.to(self.device)\n", "            correct_ids = correct_ids.to(self.device)\n\n", "        # generate mask design from latent sample\n", "        mask = (\n", "            self.generator(torch.rand(1, 100).to(self.device))\n", "            if self.device\n", "            else self.generator()\n", "        )\n\n", "        # project mask design onto images\n", "        generator_faces = self.project_mask(mask, masked_faces) - 0.5\n\n", "        # get embeddings from discriminator\n", "        generator_embeddings = self.discriminator(generator_faces)\n\n", "        # determine distances from known, unmasked embeddings\n", "        difference = self.unmasked_embeddings.unsqueeze(\n", "            0\n", "        ) - generator_embeddings.unsqueeze(1)\n", "        generator_distances = torch.linalg.norm(difference, dim=2)\n\n", "        # train generator\n", "        self.generator.optimizer.zero_grad()\n", "        generator_loss = self.generator.criterion(generator_distances, correct_ids)\n", "        generator_loss.backward(retain_graph=True)\n", "        self.generator.optimizer.step()\n\n", "        # train discriminator (unmasked)\n", "        new_unmasked_embeddings = self.discriminator(unmasked_faces)\n", "        difference = self.unmasked_embeddings.unsqueeze(\n", "            0\n", "        ) - new_unmasked_embeddings.unsqueeze(1)\n", "        unmasked_distances = torch.linalg.norm(difference, dim=2)\n", "        self.discriminator.optimizer.zero_grad()\n", "        discriminator_loss = self.discriminator.criterion(\n", "            unmasked_distances, correct_ids\n", "        )\n", "        discriminator_loss.backward(retain_graph=True)\n", "        self.discriminator.optimizer.step()\n\n", "        # train discriminator (original masks)\n", "        masked_embeddings = self.discriminator(masked_faces)\n", "        difference = self.unmasked_embeddings.unsqueeze(\n", "            0\n", "        ) - masked_embeddings.unsqueeze(1)\n", "        masked_distances = torch.linalg.norm(difference, dim=2)\n", "        self.discriminator.optimizer.zero_grad()\n", "        discriminator_loss = self.discriminator.criterion(masked_distances, correct_ids)\n", "        discriminator_loss.backward(retain_graph=True)\n", "        self.discriminator.optimizer.step()\n\n", "        # train discriminator (generator masks)\n", "        detached_faces = generator_faces.detach()\n", "        detached_embeddings = self.discriminator(detached_faces)\n", "        difference = self.unmasked_embeddings.unsqueeze(\n", "            0\n", "        ) - detached_embeddings.unsqueeze(1)\n", "        detached_distances = torch.linalg.norm(difference, dim=2)\n", "        self.discriminator.optimizer.zero_grad()\n", "        discriminator_loss = self.discriminator.criterion(\n", "            detached_distances, correct_ids\n", "        )\n", "        discriminator_loss.backward(retain_graph=True)\n", "        self.discriminator.optimizer.step()\n\n", "        # return data to CPU if necessary\n", "        if self.device:\n", "            masked_faces = masked_faces.to(torch.device(\"cpu\"))\n", "            correct_ids = correct_ids.to(torch.device(\"cpu\"))\n", "        return (\n", "            float(generator_loss) / generator_distances.shape[0],\n", "            float(discriminator_loss) / detached_distances.shape[0],\n", "        )\n", "    def evaluate(self, inputs, correct_classes, batch_size=16):\n", "        \"\"\"Evaluate classification accuracy of GAN for given inputs.\n", "        \n", "        # Arguments\n", "        * `inputs` - a set of images of faces\n", "        * `correct_classes` - the correct IDs of every face\n", "        * `batch_size` - the number of images to put in each batch, defaults to 16\n", "        \"\"\"\n", "        # set model to eval mode\n", "        self.generator.eval()\n", "        if self.device:\n", "            # compute in batches\n", "            accuracies = []\n", "            minibatches = utils.batch_data(\n", "                inputs, correct_classes, batch_size=batch_size\n", "            )\n", "            for batch in minibatches:\n", "                # collect batch and send to CUDA\n", "                batch_inputs, batch_correct_classes = batch\n", "                batch_inputs = batch_inputs.to(self.device)\n", "                batch_correct_classes = batch_correct_classes.to(self.device)\n", "                # calculate classification accuracy\n", "                batch_outputs = self.forward(batch_inputs)\n", "                batch_output_classes = torch.argmin(batch_outputs, dim=1)\n", "                accuracies.append(\n", "                    sum(batch_output_classes == batch_correct_classes)\n", "                    / len(batch_correct_classes)\n", "                )\n", "                # return batch to CPU\n", "                batch_inputs = batch_inputs.to(torch.device(\"cpu\"))\n", "                batch_correct_classes = batch_correct_classes.to(torch.device(\"cpu\"))\n", "            return sum(accuracies) / len(accuracies)\n", "        else:\n", "            # forward propagation\n", "            batch_outputs = self.forward(inputs)\n\n", "            # calculate classification accuracy\n", "            output_classes = torch.argmin(batch_outputs, dim=1)\n", "            accuracy = sum(output_classes == correct_classes) / len(correct_classes)\n", "            return accuracy\n", "    def discriminator_evaluate(self, query, target, batch_size=16):\n", "        \"\"\"Evaluate classification accuracy of discriminator for given inputs.\n", "        \n", "        # Arguments\n", "        * `query` - the input images to perform facial recognition on\n", "        * `target` - the \"known\" faces that the queried images are compared to\n", "        * `batch_size` - the number of images to put in each batch, defaults to 16\n", "        \"\"\"\n", "        if self.device:\n", "            # compute in batches\n", "            accuracies = []\n", "            minibatches = utils.batch_data(query, target, batch_size=batch_size)\n", "            for idx, batch in enumerate(minibatches):\n", "                # collect batch and send to CUDA\n", "                batch_queries, batch_targets = batch\n", "                batch_queries = batch_queries.to(self.device)\n", "                batch_targets = batch_targets.to(self.device)\n", "                # get embeddings\n", "                target_embeddings = self.forward(batch_targets)\n", "                query_embeddings = self.forward(batch_queries)\n", "                # find distances between embeddings\n", "                target_embeddings = torch.unsqueeze(target_embeddings, 1)\n", "                query_embeddings = torch.unsqueeze(query_embeddings, 0)\n", "                distances = torch.norm(target_embeddings - query_embeddings, dim=2)\n", "                # classify queried images\n", "                classifications = torch.argmin(distances, dim=0) + (idx * batch_size)\n", "                id_range = torch.Tensor(\n", "                    [(idx * batch_size) + i for i in range(batch_size)]\n", "                ).to(self.device)\n", "                accuracies.append(sum(classifications == id_range) / batch_size)\n", "                # return batch to CPU\n", "                batch_queries = batch_queries.to(torch.device(\"cpu\"))\n", "                batch_targets = batch_targets.to(torch.device(\"cpu\"))\n", "            return sum(accuracies) / len(accuracies)\n", "        else:\n", "            # get embeddings\n", "            target_embeddings = self.forward(target)\n", "            query_embeddings = self.forward(query)\n\n", "            # find distances between embeddings\n", "            target_embeddings = torch.unsqueeze(target_embeddings, 1)\n", "            query_embeddings = torch.unsqueeze(query_embeddings, 0)\n", "            distances = torch.norm(target_embeddings - query_embeddings, dim=2)\n\n", "            # classify queried images and return accuracy\n", "            num_queries = query.shape[0]\n", "            classifications = torch.argmin(distances, dim=0)\n", "            accuracy = (\n", "                sum(classifications == torch.Tensor([i for i in range(num_queries)]))\n", "                / num_queries\n", "            )\n", "            return accuracy\n", "    def save(self, dir, suffix):\n", "        \"\"\"Save all three models in `<dir>/<model>_<suffix>.pt` format.\n", "        \n", "        # Arguments\n", "        * `dir` - directory to save models in\n", "        * `suffix` - suffix to label models with\n", "        \"\"\"\n", "        # format save paths\n", "        generator_path = \"{}/generator_{}.pt\".format(dir, suffix)\n", "        projector_path = \"{}/projector_{}.pt\".format(dir, suffix)\n", "        discriminator_path = \"{}/discriminator_{}.pt\".format(dir, suffix)\n\n", "        # save\n", "        torch.save(self.generator.state_dict(), generator_path)\n", "        torch.save(self.projector.state_dict(), projector_path)\n", "        torch.save(self.discriminator.finetune.state_dict(), discriminator_path)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}